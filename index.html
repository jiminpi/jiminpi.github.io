<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<!-- saved from url=(0063)file:///D:/Git_Projects/my_website/jiminpi.github.io/index.html -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="keywords" content="Jimin Pi; Eye Tracking; Human Computer Interaction; Accessibility; Health care; Hong Kong University of Science and Technology; HKUST">
<script src="assets/js/jquery.scrolly.min.js"></script>
<script src="assets/js/jquery.min.js"></script>
<link rel="author" href="https://jiminpi.github.io/">

	<title>Jimin's Homepage</title>
	<style>

@media screen and (max-device-width: 480px){
  body{
    -webkit-text-size-adjust: none;
  }
}
p { font-size : 16px; }
h1 { font-size : 34px; margin : 0; padding : 0; }
h2 { font-size : 20px; margin : 0; padding : 0; }
h3 { font-size : 18px; margin : 8; padding : 0; }
body { padding : 0; font-family : Arial; font-size : 16px; background-color : rgb(224, 224, 224); }
.title { width : 650px; margin : 20px auto; }
.container { width : 750px; margin : 20px auto; border-radius: 10px;  background-color : #fff; padding : 20px;  clear:both;}
.container_title { width : 750px; margin : 20px auto; border-radius: 10px;  padding : 20px;  clear:both;}
.iframe_video {float: left; margin-right: 30px}
#bio {
    padding-top : 20px;
}
#me { border : 0 solid black; margin-bottom : 50px; border-radius : 10px; }
#sidebar { margin-left : 25px; margin-right : 100px; border : 0 solid black; float : left; margin-bottom : 0;}
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #0050e7; }
.publogo { width: 100 px; margin-right : 20px; float : left; border : 0;}
.publication { clear : left; padding-bottom : 0px; }
.publication p { height : 100px; padding-top : 5px;}
.publication strong a { color : #0000A0; }
.publication .links { position : relative; top : 15px }
.publication .links a { margin-right : 20px; }
.codelogo { margin-right : 10px; float : left; border : 0;}
.code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
.code .download a { display : block; margin : 0 15px; float : left;}
.code strong a { color : #000; }
.external a { margin : 0 10px; }
.external a.first { margin : 0 10px 0 0; }
	</style>
	<script async="" src="./website_elements/analytics.js"></script>
</head>

<body>
	<div class="container">
		<div id="sidebar">
            <img src="./website_elements/me.jpg" vspace="50 px" width="180 px" id="me" itemprop="photo">
        </div>
		<div id="bio">

            <br>

			<h1>
				<span itemprop="name">Jimin Pi<font size="5"></font> </span>
			</h1>

            <br>

			<p style="line-height:23px;">
				<font size="5">Ph.D. Candidate</font> 
                <br>
                <br><font size="4">Hong Kong University of Science and Technology</font><br>
                <br><font size="4"> Email: jpi (at) connect.ust.hk</font>
               
                <br>
			</p>

            <br>

            <p class="external">
                <img href="https://linkedin.com/in/jiminpi" src="website_elements/linkedin.png" width="20" height="20">
                <a href="https://linkedin.com/in/jiminpi" class="first">LinkedIn</a>
                &nbsp;&nbsp;&nbsp;&nbsp;
                <img href="https://scholar.google.com.hk/citations?user=cViuXrMAAAAJ&amp;hl=en" src="website_elements/googleScholar.png" width="25" height="25">
                <a href="https://scholar.google.com.hk/citations?user=cViuXrMAAAAJ&amp;hl=en">Google Scholar</a>
            </p>
		</div>
	</div>

	<div class="container">
		<h2>About me</h2>
		<p> 
            Welcome to my personal webpage!            
            <br><br>
            I'm a PhD student at the Hong Kong University of Science and Technology (HKUST). I investigate about system for human-computer interactions and human-robot interactions. 
            More specifically, my thesis focuses on intelligent gaze-based interaction systems.  
            <br><br> 
            I think this is an exciting field because it has many immediate applications. For instance, in one of my recent research projects I developed a keyboard controlled by the eye gaze. 
            This keyboard could, for example, help people with upper limb disabilities interact with computers. We introduced a probabilistic model for the gaze-based selection, which enabled us to incorporate letter prediction probabilistically. 
            I collaborated with public hospitals in Hong Kong to validate our keyboard. We found that the users had an increase in typing speed of about 50% compared to previous techniques.
            <br><br> 
            If you are interested in this area, you can also visit <a href="https://scholar.google.com.hk/citations?user=KaaF6ooAAAAJ&hl=en">my advisor's Google Scholar page</a>. In his profile, you can find many other exciting projects carried by my colleagues. 
            <br><br> 
            Before joining HKUST, I received my bachelor degree from the Huazhong University of Science and Technology (HUST). I had the third best GPA among 170 students. I had also been awarded the prestigious National Scholarship twice in 2011, and 2012.
            <br><br> 
            <br>
            Some of my key skill sets are: <br> 
            &nbsp; - Probabilistic machine learning <br> 
            &nbsp; - Statistical analysis <br> 
            &nbsp; - Eye tracking systems <br> 
            <br> <br> 
            Please, visit my <a href="https://linkedin.com/in/jiminpi" class="first">LinkedIn</a> to find my previous experiences. You can also follow my recent publications in my <a href="https://scholar.google.com.hk/citations?user=cViuXrMAAAAJ&amp;hl=en">Google Scholar</a>.  
		</p>
	</div>

	<!--
    <div class="container">
        <h2>News</h2>
        <p>
        </p><li> We are organizing ICCV 2019 workshop on <a href="https://sense-human.github.io/">Sensing, Understanding and Synthesizing Humans</a>. </li>
        <br>
        <li> I will give an invited talk at ICCV 2019 workshop on <a href="https://sites.google.com/view/cvcreative/home">Computer Vision for Fashion, Art and Design</a>. </li>
        <p></p>
    </div>
-->

<div class="container_title">
	<h2>Projects</h2>
</div>
<div class="container">
    <p><h2>Gaze-based Text Input System with Dynamic Bayesian Adjustment</h2></p>
    <div class="publication">
        <iframe class="iframe_video" width="300" height="200" src="https://www.youtube.com/embed/aATFK4gcykQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" style="float:left"></iframe>
        <li> From Mar 2016 to present. </li>
        <li> Implementation of a text input interface using gaze. </li>
        <li> Proposed a probabilistic generative model for gaze. </li>
        <li> Used Bayesian to integrate gaze cue and context knowledge.  </li>
        <li> Achieved 50% typing speed increase compared to previous techniques. </li>
        <li> Tested with (Spinal cord injuries) patients in Hong Kong hospitals. </li>
    </div>
    <br>
    <div style="margin-left: 20px">
        <p>
                    <br>
                    <b>Publications:</b>
                    <br>
                    <li>Dynamic Bayesian Adjustment of Dwell Time for Faster Gaze Typing.                  
                    <b>Jimin Pi</b>, Paul A. Koljonen, Yong Hu and Bertram E. SHI.
                    <em>Submitted to IEEE Transactions on Neural Systems and Rehabilitation Engineering. </em></li>
                    <br>
                    <li>Probabilistic Adjustment of Dwell Time for Eye Typing. 
                    <b>Jimin Pi</b> and Bertram E. SHI. 
                    <em>IEEE International conference on Human system interaction (HSI), 2017 <font color="#e86e14">(Best Oral Presentation Award)</font> </em>
                    <span class="links">
                        <a href="./papers/08005041.pdf">PDF</a>
                    </span>
                     </li>
        </p>
    </div>
</div>    

<div class="container">
        <p><h2>SLAM-based 3D Gaze Estimation Using Eye Glasses</h2></p>
        <div class="publication">
            <iframe class="iframe_video" width="300" height="200" src="https://www.youtube.com/embed/-dNn4fH_erk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" style="float:left"></iframe>
            <li> From Jul 2017 to Apr 2018.</li>
            <li> Estimating human pose through SLAM algorithm. </li>
            <li> Localizing 3D gaze based on the 3D environment context. </li>
            <li> Achieved accuracy of 3 degrees in a region of over 2mx2m. </li>
        </div>
        <br><br><br><br>
        <div style="margin-left: 20px">
            <p>
                        <br>
                        <b>Publication:</b>
                        <br>
                        <li>SLAM-based Localization of 3D Gaze Using A Mobile Eye Tracker.                          
                        Haofei Wang*, <b>Jimin Pi*</b>, Tong Qin, Shaojie Shen and Bertram E. SHI. 
                        <em>ACM Symposium on Eye Tracking Research & Applications (ETRA), 2018. </em>(*Equal contribution) 
                        &nbsp; 
                        <span class="links">
                            <a href="./papers/a65-wang.pdf">PDF</a>
                        </span>
                        </li>
            </p>
        </div>
    </div>  

<div class="container">
        <p><h2>Data-driven Online Calibration for Remote Eye Trackers</h2></p>
        <div class="publication">
            <img class = "iframe_video" src = "website_elements/coordinate_system.png" width="300" height="200" >
            <li> From Aug 2018 to Mar 2019.</li>
            <li> Investigated eye tracking degradation through empirical study.  </li>
            <li> Proposed a calibration mechanism based on the natural human-computer interaction history.  </li>
            <li> Solved the calibration using an efficient convex optimization scheme. </li>
            <li> Reduced errors by up to 43% as the head moves over a 20cm range </li>
        </div>
        <br>
        <div style="margin-left: 20px">
            <p>
                        <br>
                        <b>Publication:</b>
                        <br>
                        <li>Task-embedded online eye-tracker calibration for improving robustness to head motion. 
                        <b>Jimin Pi</b> and Bertram E. SHI. 
                        <em>ACM Symposium on Eye Tracking Research & Applications (ETRA), 2019. <font color="#e86e14">(Oral)</font> </em>
                        &nbsp; 
                        <span class="links">
                            <a href="./papers/a8-pi.pdf">PDF</a>
                        </span>
                        </li>
            </p>
        </div>
    </div>  

<div class="container">
        <p><h2>Unsupervised Outlier Detection in Appearance-Based Gaze Estimation</h2></p>
        <div class="publication">
            <img class = "iframe_video" src = "website_elements/outlier_detection.jpg" width="300" height="120" >
            <li> From Jun 2019-Aug 2019.</li>
            <li> Unsupervisely detect outlier inputs during training of the gaze estimator.  </li>
            <li> Reduced error by 8% compared to model without outlier detection.   </li>
            <li> Detect outliers with precision 0.71 when the recall is 0.63. </li>
        </div>
        <div style="margin-left: 20px">
            <p>
                        <br>
                        <b>Publication:</b>
                        <br>
                        Unsupervised Outlier Detection in Appearance-Based Gaze Estimation.
                        <br>
                        Zhaokang Chen*, Didan Deng*, <b>Jimin Pi*</b>, and Bertram E. SHI.  (*Equal contribution)
                        <br>
                        <em>ICCV 2019 Workshop and Challenge on Real-World Recognition from Low-Quality Images and Videos. </em>
            </p>
        </div>
    </div>  

<div class="container">
        <p><h2>Pose-independent Facial Expression Recognition</h2></p>
        <div class="publication">
            <img class = "iframe_video" src = "website_elements/FG.jpg" width="300" height="130" >
            <li> From Dec 2016 to Feb 2017.</li>
            <li> Got the <font color="#e86e14">winner</font> of Action Unit Intensity Estimation sub-challenge in the Facial Expression Recognition and Analysis Challenge (FERA 2017). </li>
            <li> Proposed a multi-task deep network addressing different pose angles.    </li>
            <li> Achieved balanced performance among nine pose angles for most AUs. </li>
        </div>
        <div style="margin-left: 20px">
            <p>
                        <br>
                        <b>Publication:</b>
                        <br>
                        Pose-independent Facial Action Unit Intensity Regression Based on Multi-task Deep Transfer Learning.
                        <br>
                        Yuqian Zhou, <b>Jimin Pi</b>, and Bertram E. SHI.  
                        <br>
                        <em>IEEE International Conference on Automatic Face & Gesture Recognition (FG), 2017. </em>
                        &nbsp; 
                        <span class="links">
                            <a href="./papers/07961835.pdf">PDF</a>
                        </span>
            </p>
        </div>
    </div> 

<div class="container_title">
    <h2>Activities</h2>
</div>


<div class="container">
    <li> I served as a reviewer and PC member of ACM conference ETRA in 2018 and 2019. I was also a volunteer for the ACM Siggraph Asia 2014.</li>
    <li>I served as teaching assistant for the PG course
    "Multimedia Signal Processing" in 2016 spring and the UG course ``A System View of Communications: from Signals to Packets" in 2014 spring and fall. </li>
    </div>
</div>

<div class = "container_title">
    <p style="margin-bottom: 4px" ><strong>Visitors</strong></p>
    <div style="max-width: 300px; margin: 0px auto;">
    <script type="text/javascript" src="//rf.revolvermaps.com/0/0/7.js?i=5c7d8dz5ge5&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;sx=0" async="async"></script>
    </div>
</div>

</body></html>